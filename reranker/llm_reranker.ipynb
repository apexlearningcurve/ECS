{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gpt_reranker import run_reranker\n",
    "from openai import OpenAI\n",
    "from prompts import AMAZON_RANKING_PROMPT\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT Reranker - Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_text = \"\"\"\n",
    "mCover Case Compatible ONLY for 2021～2023 14\" Dell Latitude 5420 5430 Windows Notebook Computer (NOT Fitting Any Other Dell Models) - Pink\n",
    "\n",
    "product category: Electronics\n",
    "\"\"\"\n",
    "query = \"apple laptop case\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_text = \"\"\"\n",
    "YMIX Macbook Pro 13\" Case Non-Retina,Folio Embroidered Shell Plastic Hard Protective Cover for Old MacBook Pro 13 Inch with CD-ROM Drive,Model A1278(A_Embroidered Floral)\n",
    "\n",
    "product category: Electronics\n",
    "\"\"\"\n",
    "query = \"apple laptop case\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-24 16:50:06.676\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mgpt_reranker\u001b[0m:\u001b[36mrun_reranker\u001b[0m:\u001b[36m77\u001b[0m - \u001b[32m\u001b[1mLabel: No with Probability: 1.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "label, probab = run_reranker(\n",
    "    client=client,\n",
    "    prompt=AMAZON_RANKING_PROMPT,\n",
    "    query=query,\n",
    "    product_text=product_text,\n",
    "    logger_level=\"SUCCESS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-24 16:50:14.664\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mgpt_reranker\u001b[0m:\u001b[36mrun_reranker\u001b[0m:\u001b[36m77\u001b[0m - \u001b[32m\u001b[1mLabel: No with Probability: 1.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "label, probab = run_reranker(\n",
    "    client=client,\n",
    "    prompt=AMAZON_RANKING_PROMPT,\n",
    "    query=query,\n",
    "    product_text=product_text,\n",
    "    logger_level=\"SUCCESS\",\n",
    "    logit_bias_value=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OS - Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "cache_dir = Path(\"../cache\")\n",
    "assert cache_dir.exists(), f\"Cache directory {cache_dir} does not exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4419ade4e245e78e806f89a7c73120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=cache_dir)\n",
    "quantization_config = QuantoConfig(weights=\"int8\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    cache_dir=cache_dir,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID for Yes: 9642, No: 2822\n"
     ]
    }
   ],
   "source": [
    "yes_token_id = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "no_token_id = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "bias_tokens = [yes_token_id, no_token_id]\n",
    "print(f\"ID for Yes: {yes_token_id}, No: {no_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_text = \"\"\"\n",
    "mCover Case Compatible ONLY for 2021～2023 14\" Dell Latitude 5420 5430 Windows Notebook Computer (NOT Fitting Any Other Dell Models) - Pink\n",
    "\n",
    "product category: Electronics\n",
    "\"\"\"\n",
    "query = \"laptop case\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = AMAZON_RANKING_PROMPT.format(query=query, product_text=product_text)\n",
    "# outputs = product_relevance(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Note\n",
      "Score for Yes: 0.03128882497549057, No: 0.024755867198109627\n"
     ]
    }
   ],
   "source": [
    "def custom_logits_processor(_, logits):\n",
    "\n",
    "    for token_id in bias_tokens:\n",
    "        print(f\"Logits value for token ID {token_id}: {logits[0, token_id]}\")\n",
    "        logits[0, token_id] += bias_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "bias_value = 1\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "prompt_token_num = inputs.input_ids[0].shape[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,  # We're using greedy decoding\n",
    "        num_beams=1,  # Ensure we're not doing beam search\n",
    "        temperature=None,\n",
    "        top_p=None,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        # logits_processor=[custom_logits_processor],\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "label_token = outputs[0][0][prompt_token_num:].item()\n",
    "label = tokenizer.decode(label_token)\n",
    "print(f\"Label: {label}\")\n",
    "\n",
    "scores = outputs.scores[0]\n",
    "# Maybe do softmax only on yes and no scores\n",
    "scores = torch.softmax(scores, dim=-1)\n",
    "yes_score = scores[0, yes_token_id].item()\n",
    "no_score = scores[0, no_token_id].item()\n",
    "print(f\"Score for Yes: {yes_score}, No: {no_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5016, 0.4984]], device='mps:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(scores[:, [yes_token_id, no_token_id]], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.scores[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score for Yes: 0.0775807648897171, No: 0.061382267624139786\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.scores[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "You are an Assistant responsible for helping detect whether the retrieved product is relevant to the query. For a given input, you need to output a single token: \"Yes\" or \"No\" indicating the retrieved product is relevant to the query.\n",
      "\n",
      "Query: Younique setting powder\n",
      "Product: \n",
      "\"\"\"\n",
      "Younique Touch Behold Translucent Setting Powder\n",
      "\n",
      "Touch Behold Translucent Setting Powder. Younique’s Touch Behold Translucent Setting Powder effortlessly locks and loads your look so you’re ready to take on the world. Use as the finishing touch to help keep makeup in place, or wear directly on skin for a softening, matte look.\n",
      "\n",
      "product category: Beauty & Personal Care\n",
      "\"\"\"\n",
      "Relevant: Yes\n",
      "\n",
      "Query: white musk hand cream\n",
      "Product: \n",
      "\"\"\"\n",
      "Braided Hair Clips for Women Girls, Sparkling Crystal Stone Braided Hair Clips Barrette with 3 Small Clips, Triple Hair Clips with Rhinestones for Sectioning,4PCS (4pcs-Type A)\n",
      "\n",
      "product category: Beauty & Personal Care\n",
      "\"\"\"\n",
      "Relevant: No\n",
      "\n",
      "Query: HP Pavilion dm4 replacement battery\n",
      "Product: \n",
      "\"\"\"\n",
      "ATC 11.1V 6-Cell Replacement Laptop Battery for HP Pavilion dm4-1062nr Pavilion dm4-1063cl Pavilion dm4-1063he Pavilion dm4-1065dx Pavilion dm4-1070ee Pavilion dm4-1070ef\n",
      "\n",
      "product category: Electronics\n",
      "\"\"\"\n",
      "Relevant: Yes\n",
      "\n",
      "Query: Cushionaire cork sandals\n",
      "Product: \n",
      "\"\"\"\n",
      "CUSHIONAIRE Women's Lane Cozy Cork footbed Sandal with Faux fur lining and +Comfort\n",
      "\n",
      "Women's Cushionaire comfort Cork footbed sandal with Faux Fur lining. Stay cool with comfy sandals that will give you comfort throughout your day.\n",
      "\n",
      "product category: Clothing Shoes & Jewelry\n",
      "\"\"\"\n",
      "Relevant: Yes\n",
      "\n",
      "Query: under cabinet LED light\n",
      "Product: \n",
      "\"\"\"\n",
      "Christmas Snowflake Projector Lights Outdoor Led Snowfall Show with Remote Control Waterproof Landscape Decorative Lighting for Christmas Holiday Party Wedding Garden Patio\n",
      "\n",
      "product category: Tools & Home Improvement\n",
      "\"\"\"\n",
      "Relevant: No \n",
      "\n",
      "Query: macbook keyboard\n",
      "Product: \n",
      "\"\"\"\n",
      "\n",
      "YMIX Macbook Pro 13\" Case Non-Retina,Folio Embroidered Shell Plastic Hard Protective Cover for Old MacBook Pro 13 Inch with CD-ROM Drive,Model A1278(A_Embroidered Floral)\n",
      "\n",
      "product category: Electronics\n",
      "\n",
      "\"\"\"\n",
      "Relevant: \n",
      "The input consists of two\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\n",
    "    AMAZON_RANKING_PROMPT.format(query=\"macbook keyboard\", product_text=product_text),\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=5)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID for Yes: 9642, No: 2822\n"
     ]
    }
   ],
   "source": [
    "yes_token_id = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "no_token_id = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "print(f\"ID for Yes: {yes_token_id}, No: {no_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = tokenizer.encode(\n",
    "    AMAZON_RANKING_PROMPT.format(query=query, product_text=product_text),\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "output = model.forward(input_tensor)\n",
    "logits = output.logits[0, -1]  # from first batch take last token logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability for Yes: 0.010473666712641716\n",
      "Probability for No: 0.19977031648159027\n"
     ]
    }
   ],
   "source": [
    "print(f\"Probability for Yes: {torch.softmax(logits, dim=-1)[yes_token_id]}\")\n",
    "print(f\"Probability for No: {torch.softmax(logits, dim=-1)[no_token_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score for Yes: 0.010473666712641716, No: 0.19977031648159027\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability for Yes: 0.668552815914154\n",
      "Probability for No: 0.09124583750963211\n"
     ]
    }
   ],
   "source": [
    "print(f\"Probability for Yes: {torch.softmax(logits, dim=-1)[yes_token_id]}\")\n",
    "print(f\"Probability for No: {torch.softmax(logits, dim=-1)[no_token_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
